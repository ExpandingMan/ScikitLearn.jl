var documenterSearchIndex = {"docs":
[{"location":"man/jld/#Saving-models-to-disk-1","page":"Saving Models to Disk","title":"Saving models to disk","text":"","category":"section"},{"location":"man/jld/#","page":"Saving Models to Disk","title":"Saving Models to Disk","text":"JLD.jl is the preferred way of saving ScikitLearn.jl models. If you also use Python models (via @sk_import), you will have to import PyCallJLD as well.","category":"page"},{"location":"man/jld/#","page":"Saving Models to Disk","title":"Saving Models to Disk","text":"using PyCall, JLD, PyCallJLD\nusing ScikitLearn\nusing ScikitLearn.Pipelines\n@sk_import decomposition: PCA\n@sk_import linear_model: LinearRegression\n\npca = PCA()\nlm = LinearRegression()\n\nX=rand(10, 3); y=rand(10);\n\npip = Pipeline([(\"PCA\", pca), (\"LinearRegression\", lm)])\nfit!(pip, X, y)   # fit to some dataset\n\nJLD.save(\"pipeline.jld\", \"pip\", pip)\npip = JLD.load(\"pipeline.jld\", \"pip\") # Load back the pipeline\n","category":"page"},{"location":"man/cross_validation/#Cross-validation-1","page":"Cross-validation","title":"Cross-validation","text":"","category":"section"},{"location":"man/cross_validation/#","page":"Cross-validation","title":"Cross-validation","text":"DocTestSetup = quote\n    using ScikitLearn\nusing RDatasets: dataset\n@sk_import linear_model: LogisticRegression\niris = dataset(\"datasets\", \"iris\")\nX = convert(Array, iris[!, [:SepalLength, :SepalWidth, :PetalLength,\n        :PetalWidth]])\ny = convert(Array, iris[!,:Species])\nend","category":"page"},{"location":"man/cross_validation/#","page":"Cross-validation","title":"Cross-validation","text":"Cross-validation in ScikitLearn.jl is the same as in scikit-learn:","category":"page"},{"location":"man/cross_validation/#","page":"Cross-validation","title":"Cross-validation","text":"using ScikitLearn\nusing RDatasets: dataset\n@sk_import linear_model: LogisticRegression\niris = dataset(\"datasets\", \"iris\")\nX = convert(Array, iris[!, [:SepalLength, :SepalWidth, :PetalLength, \n        :PetalWidth]])\ny = convert(Array, iris[!,:Species])\n","category":"page"},{"location":"man/cross_validation/#","page":"Cross-validation","title":"Cross-validation","text":"using ScikitLearn.CrossValidation: cross_val_score\n\ncross_val_score(LogisticRegression(max_iter=150), X, y; cv=5)  # 5-fold\n","category":"page"},{"location":"man/cross_validation/#","page":"Cross-validation","title":"Cross-validation","text":"See ?cross_val_score and the user guide for details.","category":"page"},{"location":"man/cross_validation/#","page":"Cross-validation","title":"Cross-validation","text":"We support all the scikit-learn cross-validation iterators (KFold, StratifiedKFold, etc.) For example:","category":"page"},{"location":"man/cross_validation/#","page":"Cross-validation","title":"Cross-validation","text":"julia> ScikitLearn.CrossValidation.KFold(10, n_folds=3)\n3-element Array{Tuple{Array{Int64,1},Array{Int64,1}},1}:\n ([5, 6, 7, 8, 9, 10], [1, 2, 3, 4])\n ([1, 2, 3, 4, 8, 9, 10], [5, 6, 7])\n ([1, 2, 3, 4, 5, 6, 7], [8, 9, 10])\n","category":"page"},{"location":"man/cross_validation/#","page":"Cross-validation","title":"Cross-validation","text":"These iterators can be passed to cross_val_score's cv argument.","category":"page"},{"location":"man/cross_validation/#","page":"Cross-validation","title":"Cross-validation","text":"Note: the most common iterators have been translated to Julia. The others still require scikit-learn (python) to be installed.","category":"page"},{"location":"man/cross_validation/#Examples-1","page":"Cross-validation","title":"Examples","text":"","category":"section"},{"location":"man/cross_validation/#","page":"Cross-validation","title":"Cross-validation","text":"Quick start guide\nConcatenating multiple feature extraction methods\nUnderfitting vs. Overfitting","category":"page"},{"location":"man/cross_validation/#Cross-validated-predictions-1","page":"Cross-validation","title":"Cross-validated predictions","text":"","category":"section"},{"location":"man/cross_validation/#","page":"Cross-validation","title":"Cross-validation","text":"cross_val_predict performs cross-validation and returns the test-set predicted values. Documentation here","category":"page"},{"location":"man/cross_validation/#Examples-2","page":"Cross-validation","title":"Examples","text":"","category":"section"},{"location":"man/cross_validation/#","page":"Cross-validation","title":"Cross-validation","text":"CrossValidatedPredictions","category":"page"},{"location":"man/cross_validation/#","page":"Cross-validation","title":"Cross-validation","text":"DocTestSetup = nothing","category":"page"},{"location":"man/examples/#","page":"-","title":"-","text":"The examples below are written in Julia, there are many more in Python. Most examples use Python models, those that use Julia models are marked as such. ","category":"page"},{"location":"man/examples/#","page":"-","title":"-","text":"Example Description\n<a href=\"../examples/ClassifierComparison.ipynb\"><img src=\"../docs/exampleimages/Classifier_Comparison.png\" alt=\"# Classifier Comparison\" width=\"170\"> </a> Classifier Comparison: A comparison of a several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets.  Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers.  The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.\n<a href=\"../examples/ClassifierComparisonJulia.ipynb\"><img src=\"../docs/exampleimages/ClassifierComparison_Julia.png\" alt=\"# Classifier Comparison (Julia classifiers)\" width=\"170\"> </a> Classifier Comparison (Julia classifiers): A comparison of a several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets.  Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers.  The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.\n<a href=\"../examples/ClusteringComparison.ipynb\"><img src=\"../docs/exampleimages/Clustering_Comparison.png\" alt=\"# Comparing different clustering algorithms on toy datasets\" width=\"170\"> </a> Comparing different clustering algorithms on toy datasets: This example aims at showing characteristics of different clustering algorithms on datasets that are interesting but still in 2D. The last dataset is an example of a null situation for clustering: the data is homogeneous, and there is no good clustering.  While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.  The results could be improved by tweaking the parameters for each clustering strategy, for instance setting the number of clusters for the methods that needs this parameter specified. Note that affinity propagation has a tendency to create many clusters. Thus in this example its two parameters (damping and per-point preference) were set to to mitigate this behavior.\n<a href=\"../examples/CrossValidatedPredictions.ipynb\"><img src=\"../docs/exampleimages/CrossValidated_Predictions.png\" alt=\"# Plotting Cross-Validated Predictions\" width=\"170\"> </a> Plotting Cross-Validated Predictions: This example shows how to use crossvalpredict to visualize prediction errors.\n<a href=\"../examples/DecisionTreeRegression.ipynb\"><img src=\"../docs/exampleimages/DecisionTree_Regression.png\" alt=\"# Decision Tree Regression\" width=\"170\"> </a> Decision Tree Regression: using ScikitLearn  # Import the necessary modules and libraries @skimport tree: DecisionTreeRegressor using PyPlot  # Create a random dataset srand(42) X = sort(5 * rand(80)) XX = reshape(X, 80, 1) y = sin(X) y[1:5:end] += 3 * (0.5 - rand(16))  # Fit regression model regr1 = DecisionTreeRegressor(maxdepth=2) regr2 = DecisionTreeRegressor(maxdepth=5) fit!(regr1, XX, y) fit!(regr2, XX, y)  # Predict Xtest = 0:0.01:5.0 y1 = predict(regr1, Xtest'') y2 = predict(regr2, Xtest'')  # Plot the results scatter(X, y, c=\"k\", label=\"data\") plot(Xtest, y1, c=\"g\", label=\"maxdepth=2\", linewidth=2) plot(Xtest, y2, c=\"r\", label=\"maxdepth=5\", linewidth=2) xlabel(\"data\") ylabel(\"target\") title(\"Decision Tree Regression\") legend() show()\n<a href=\"../examples/DecisionTreeRegressionJulia.ipynb\"><img src=\"../docs/exampleimages/DecisionTreeRegression_Julia.png\" alt=\"# Decision Tree Regression (using DecisionTrees.jl)\" width=\"170\"> </a> Decision Tree Regression (using DecisionTrees.jl): A 1D regression with decision tree.  The decision trees is used to fit a sine curve with addition noisy observation. As a result, it learns local linear regressions approximating the sine curve.  We can see that if the maximum depth of the tree (controlled by the max_depth parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit.\n<a href=\"../examples/DensityEstimation.ipynb\"><img src=\"../docs/exampleimages/Density_Estimation.png\" alt=\"# Density Estimation for a mixture of Gaussians\" width=\"170\"> </a> Density Estimation for a mixture of Gaussians: Plot the density estimation of a mixture of two Gaussians. Data is generated from two Gaussians with different centers and covariance matrices.\n<a href=\"../examples/DensityEstimationJulia.ipynb\"><img src=\"../docs/exampleimages/DensityEstimation_Julia.png\" alt=\"# Density Estimation for a mixture of Gaussians (using GaussianMixtures.jl)\" width=\"170\"> </a> Density Estimation for a mixture of Gaussians (using GaussianMixtures.jl): Plot the density estimation of a mixture of two Gaussians, then use cross-validation to find the optimal number of clusters. Data is generated from two Gaussians with different centers and covariance matrices. We then use GridSearchCV to find the optimal number of mixtures.\n<a href=\"../examples/FeatureStacker.ipynb\"><img src=\"../docs/exampleimages/Text_image.png\" alt=\"# Concatenating multiple feature extraction methods\" width=\"170\"> </a> Concatenating multiple feature extraction methods: In many real-world examples, there are many ways to extract features from a dataset. Often it is beneficial to combine several methods to obtain good performance. This example shows how to use FeatureUnion to combine features obtained by PCA and univariate selection.  Combining features using this transformer has the benefit that it allows cross validation and grid searches over the whole process.  The combination used in this example is not particularly helpful on this dataset and is only used to illustrate the usage of FeatureUnion.\n<a href=\"../examples/GaussianProcessesJulia.ipynb\"><img src=\"../docs/exampleimages/GaussianProcesses_Julia.png\" alt=\"# Gaussian Processes (Julia)\" width=\"170\"> </a> Gaussian Processes (Julia): using ScikitLearn using ScikitLearn.GridSearch using PyPlot using GaussianProcesses: GP, MeanZero, SE  srand(42) # Training data n = 10 x = 2 * rand(n, 1) y = sin(x[:, 1]) + 0.05*randn(n)\n<a href=\"../examples/OutlierDetection.ipynb\"><img src=\"../docs/exampleimages/Outlier_Detection.png\" alt=\"# Outlier detection with several methods\" width=\"170\"> </a> Outlier detection with several methods: When the amount of contamination is known, this example illustrates two different ways of performing Novelty and Outlier Detection:  * based on a robust estimator of covariance, which is assuming that the data are Gaussian distributed and performs better than the One-Class SVM in that case. * using the One-Class SVM and its ability to capture the shape of the data set, hence performing better when the data is strongly non-Gaussian, i.e. with two well-separated clusters;  The ground truth about inliers and outliers is given by the points colors while the orange-filled area indicates which points are reported as inliers by each method.  Here, we assume that we know the fraction of outliers in the datasets. Thus rather than using the predict method of the objects, we set the threshold on the decision_function to separate out the corresponding fraction.\n<a href=\"../examples/PipelinePCALogistic.ipynb\"><img src=\"../docs/exampleimages/PipelinePCA_Logistic.png\" alt=\"# Pipelining: chaining a PCA and a logistic regression\" width=\"170\"> </a> Pipelining: chaining a PCA and a logistic regression: The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.  We use a GridSearchCV to set the dimensionality of the PCA\n<a href=\"../examples/PlotKmeansDigits.ipynb\"><img src=\"../docs/exampleimages/PlotKmeans_Digits.png\" alt=\"# A demo of K-Means clustering on the handwritten digits data\" width=\"170\"> </a> A demo of K-Means clustering on the handwritten digits data: In this example we compare the various initialization strategies for K-means in terms of runtime and quality of the results.  As the ground truth is known here, we also apply different cluster quality metrics to judge the goodness of fit of the cluster labels to the ground truth.\n<a href=\"../examples/PlotKmeansDigitsJulia.ipynb\"><img src=\"../docs/exampleimages/PlotKmeansDigits_Julia.png\" alt=\"# A demo of K-Means clustering on the handwritten digits data (using LowRankModels.jl)\" width=\"170\"> </a> A demo of K-Means clustering on the handwritten digits data (using LowRankModels.jl): In this example we apply K-Means to the digits dataset.  As the ground truth is known here, we also apply different cluster quality metrics to judge the goodness of fit of the cluster labels to the ground truth.  IMPORTANT NOTE: LowRankModels.jl's PCA is correct (though slower than conventional algorithms), but its K-Means does not always converge, as can be seen in the last figure. It's an ongoing effort.\n<a href=\"../examples/RandomizedSearch.ipynb\"><img src=\"../docs/exampleimages/Text_image.png\" alt=\"# Comparing randomized search and grid search for hyperparameter estimation\" width=\"170\"> </a> Comparing randomized search and grid search for hyperparameter estimation: Compare randomized search and grid search for optimizing hyperparameters of a random forest. All parameters that influence the learning are searched simultaneously (except for the number of estimators, which poses a time / quality tradeoff).  The randomized search and the grid search explore exactly the same space of parameters. The result in parameter settings is quite similar, while the run time for randomized search is drastically lower.  The performance is slightly worse for the randomized search, though this is most likely a noise effect and would not carry over to a held-out test set.  Note that in practice, one would not search over this many different parameters simultaneously using grid search, but pick only the ones deemed most important.\n<a href=\"../examples/RBM.ipynb\"><img src=\"../docs/example_images/RBM.png\" alt=\"# Restricted Boltzmann Machine features for digit classification\" width=\"170\"> </a> Restricted Boltzmann Machine features for digit classification: For greyscale image data where pixel values can be interpreted as degrees of blackness on a white background, like handwritten digit recognition, the Bernoulli Restricted Boltzmann machine model (BernoulliRBM) can perform effective non-linear feature extraction.  In order to learn good latent representations from a small dataset, we artificially generate more labeled data by perturbing the training data with linear shifts of 1 pixel in each direction.  This example shows how to build a classification pipeline with a BernoulliRBM feature extractor and a LogisticRegression classifier. The hyperparameters of the entire model (learning rate, hidden layer size, regularization) were optimized by grid search, but the search is not reproduced here because of runtime constraints.  Logistic regression on raw pixel values is presented for comparison. The example shows that the features extracted by the BernoulliRBM help improve the classification accuracy.\n<a href=\"../examples/Simple1DKernelDensity.ipynb\"><img src=\"../docs/exampleimages/Simple1DKernel_Density.png\" alt=\"# Simple 1D Kernel Density Estimation\" width=\"170\"> </a> Simple 1D Kernel Density Estimation: The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.  We use a GridSearchCV to set the dimensionality of the PCA\n<a href=\"../examples/TextFeatureExtraction.ipynb\"><img src=\"../docs/exampleimages/Textimage.png\" alt=\"# Sample pipeline for text feature extraction and evaluation\" width=\"170\"> </a> Sample pipeline for text feature extraction and evaluation: The dataset used in this example is the 20 newsgroups dataset which will be automatically downloaded and then cached and reused for the document classification example.  You can adjust the number of categories by giving their names to the dataset loader or setting them to None to get the 20 of them.\n<a href=\"../examples/TwoClassAdaboost.ipynb\"><img src=\"../docs/exampleimages/TwoClass_Adaboost.png\" alt=\"# Two Class Adaboost\" width=\"170\"> </a> Two Class Adaboost: This example fits an AdaBoosted decision stump on a non-linearly separable classification dataset composed of two Gaussian quantiles clusters (see sklearn.datasets.make_gaussian_quantiles) and plots the decision boundary and decision scores. The distributions of decision scores are shown separately for samples of class A and B. The predicted class label for each sample is determined by the sign of the decision score. Samples with decision scores greater than zero are classified as B, and are otherwise classified as A. The magnitude of a decision score determines the degree of likeness with the predicted class label. Additionally, a new dataset could be constructed containing a desired purity of class B, for example, by only selecting samples with a decision score above some value.\n<a href=\"../examples/UnderfittingvsOverfitting.ipynb\"><img src=\"../docs/exampleimages/Underfittingvs_Overfitting.png\" alt=\"# Underfitting vs. Overfitting\" width=\"170\"> </a> Underfitting vs. Overfitting: This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called underfitting. A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will overfit the training data, i.e. it learns the noise of the training data. We evaluate quantitatively overfitting / underfitting by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data.","category":"page"},{"location":"man/api/#Model-Api-1","page":"Model API","title":"Model Api","text":"","category":"section"},{"location":"man/api/#","page":"Model API","title":"Model API","text":"fit!fit!(model, X) ; fit!(model, X, y)Trains model on the input data X and y (for supervised learning) or on just X (for unsupervised learning).  The model object is always returned, allowing code like classifier = fit!(LogisticRegression(), X, y)","category":"page"},{"location":"man/api/#","page":"Model API","title":"Model API","text":"partial_fit!partial_fit!(model, X) ; partial_fit!(model, X, y)Incrementally trains model on the new data X and y. For instance, this might perform a stochastic gradient descent step.","category":"page"},{"location":"man/api/#","page":"Model API","title":"Model API","text":"predictpredict(model, X) returns the predicted class of each row in X (for classifiers) or the predicted value (for regressors).","category":"page"},{"location":"man/api/#","page":"Model API","title":"Model API","text":"predict_probapredict_proba(model, X) returns an (N, C) matrix containing the probability that the nth sample belongs to the cth class. Call get_classes(model) to get the ordering of the classes.","category":"page"},{"location":"man/api/#","page":"Model API","title":"Model API","text":"predictlogprobapredict_log_proba(model, X) is equivalent to log(predict_proba(model, X)) but can be more accurate (for small probabilities) and faster (avoiding the exponential).","category":"page"},{"location":"man/api/#","page":"Model API","title":"Model API","text":"transformFor unsupervised learning models and for preprocessing, transform(model, X) applies the transformation from model to X, and returns a similar array (same number of rows, possibly different number of columns).","category":"page"},{"location":"man/api/#","page":"Model API","title":"Model API","text":"get_componentsFor unsupervised learning models, get_components(model) returns the matrix of the latent space, in (ncomponents, nfeatures) form. For matrix factorization methods, this corresponds to the principal components or latent vectors.","category":"page"},{"location":"man/api/#","page":"Model API","title":"Model API","text":"fit_transform!fit_transform!(model, X) is equivalent to transform(fit!(model, X), X) but can sometimes be more efficient.","category":"page"},{"location":"man/api/#","page":"Model API","title":"Model API","text":"fit_predict!fit_predict!(model, X) is equivalent to predict(fit!(model, X), X) but can sometimes be more efficient.","category":"page"},{"location":"man/api/#","page":"Model API","title":"Model API","text":"inverse_transforminverse_transform(model, X) applies the inverse of the model transformation.","category":"page"},{"location":"man/api/#","page":"Model API","title":"Model API","text":"score_samplesFor probabilistic models, score_samples(model, X) evaluates the density model on X.","category":"page"},{"location":"man/api/#","page":"Model API","title":"Model API","text":"scorescore(model, X) and score(model, X, y) assign a score to how likely X or y|X is given the learned model parameters. The higher this score is, the better the model. This is used for cross-validation.","category":"page"},{"location":"man/api/#","page":"Model API","title":"Model API","text":"decision_functiondecision_function(model, X) returns the distance of the samples to the decision boundary.","category":"page"},{"location":"man/api/#Model-Internals-1","page":"Model API","title":"Model Internals","text":"","category":"section"},{"location":"man/api/#","page":"Model API","title":"Model API","text":"clone(model) returns a new object of the same type as model, with the same hyperparameters, but unfit.\nset_params!(model, param1=value1, param2=value2, ...) changes the model hyperparameters.\nget_params(model) returns all the model hyperparameters that can be changed with set_params!\nis_classifier(model) is true if model is a classifier.\nget_feature_names(model) returns the name of the output features\nget_classes(model) returns the label of each class","category":"page"},{"location":"man/python/#Reading-the-Scikit-Learn-Documentation.-1","page":"Reading the Scikit-learn Documentation","title":"Reading the Scikit-Learn Documentation.","text":"","category":"section"},{"location":"man/python/#Relationship-to-scikit-learn-1","page":"Reading the Scikit-learn Documentation","title":"Relationship to scikit-learn","text":"","category":"section"},{"location":"man/python/#","page":"Reading the Scikit-learn Documentation","title":"Reading the Scikit-learn Documentation","text":"ScikitLearn.jl aims to mirror the Python scikit-learn project, but the API had to be adapted to Julia, and follows Julia's conventions. When reading the Python documentation, keep in mind:","category":"page"},{"location":"man/python/#","page":"Reading the Scikit-learn Documentation","title":"Reading the Scikit-learn Documentation","text":"Most object methods are now functions: Python's model.predict(X) becomes predict(model, X)\nMethods that modify the model's state have a ! at the end: model.fit_transform(X) becomes fit_transform!(model, X)\nA few of the Python submodules were translated into Julia to support Julia models: ScikitLearn.Pipelines, ScikitLearn.CrossValidation, and ScikitLearn.GridSearch","category":"page"},{"location":"man/python/#","page":"Reading the Scikit-learn Documentation","title":"Reading the Scikit-learn Documentation","text":"You can access the class members and methods of a Python object (i.e. all models imported through @sk_import) using obj.member_name. For example:","category":"page"},{"location":"man/python/#","page":"Reading the Scikit-learn Documentation","title":"Reading the Scikit-learn Documentation","text":"using ScikitLearn #hide\nX=rand(10,4);y=rand(10);\n\n@sk_import linear_model: Lasso\nlm = fit!(Lasso(), X, y)\nprintln(lm.n_iter_)\n","category":"page"},{"location":"man/python/#","page":"Reading the Scikit-learn Documentation","title":"Reading the Scikit-learn Documentation","text":"This is rarely necessary, because the most important/frequently-used methods have been defined in Julia (eg. transformer.classes_ is now get_classes(transformer))","category":"page"},{"location":"man/quickstart/#Quick-start-guide-1","page":"Quick Start Guide","title":"Quick start guide","text":"","category":"section"},{"location":"man/quickstart/#Quick-Example-1","page":"Quick Start Guide","title":"Quick Example","text":"","category":"section"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"Let's build a classifier for the classic iris dataset. If you don't have RDatasets, Pkg.add it.","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"julia> using RDatasets: dataset\n\njulia> iris = dataset(\"datasets\", \"iris\")\n150×5 DataFrames.DataFrame\n│ Row │ SepalLength │ SepalWidth │ PetalLength │ PetalWidth │ Species   │\n│     │ Float64     │ Float64    │ Float64     │ Float64    │ Cat…      │\n├─────┼─────────────┼────────────┼─────────────┼────────────┼───────────┤\n│ 1   │ 5.1         │ 3.5        │ 1.4         │ 0.2        │ setosa    │\n│ 2   │ 4.9         │ 3.0        │ 1.4         │ 0.2        │ setosa    │\n│ 3   │ 4.7         │ 3.2        │ 1.3         │ 0.2        │ setosa    │\n│ 4   │ 4.6         │ 3.1        │ 1.5         │ 0.2        │ setosa    │\n│ 5   │ 5.0         │ 3.6        │ 1.4         │ 0.2        │ setosa    │\n│ 6   │ 5.4         │ 3.9        │ 1.7         │ 0.4        │ setosa    │\n│ 7   │ 4.6         │ 3.4        │ 1.4         │ 0.3        │ setosa    │\n⋮\n│ 143 │ 5.8         │ 2.7        │ 5.1         │ 1.9        │ virginica │\n│ 144 │ 6.8         │ 3.2        │ 5.9         │ 2.3        │ virginica │\n│ 145 │ 6.7         │ 3.3        │ 5.7         │ 2.5        │ virginica │\n│ 146 │ 6.7         │ 3.0        │ 5.2         │ 2.3        │ virginica │\n│ 147 │ 6.3         │ 2.5        │ 5.0         │ 1.9        │ virginica │\n│ 148 │ 6.5         │ 3.0        │ 5.2         │ 2.0        │ virginica │\n│ 149 │ 6.2         │ 3.4        │ 5.4         │ 2.3        │ virginica │\n│ 150 │ 5.9         │ 3.0        │ 5.1         │ 1.8        │ virginica │\n ","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"ScikitLearn.jl expects arrays, but DataFrames can also be used - see the corresponding section of the manual","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"julia> X = convert(Array, iris[!, [:SepalLength, :SepalWidth, :PetalLength, :PetalWidth]]) #subset of original data not a copy\n150×4 Array{Float64,2}:\n 5.1  3.5  1.4  0.2\n 4.9  3.0  1.4  0.2\n 4.7  3.2  1.3  0.2\n 4.6  3.1  1.5  0.2\n 5.0  3.6  1.4  0.2\n 5.4  3.9  1.7  0.4\n 4.6  3.4  1.4  0.3\n 5.0  3.4  1.5  0.2\n 4.4  2.9  1.4  0.2\n 4.9  3.1  1.5  0.1\n ⋮\n 6.9  3.1  5.1  2.3\n 5.8  2.7  5.1  1.9\n 6.8  3.2  5.9  2.3\n 6.7  3.3  5.7  2.5\n 6.7  3.0  5.2  2.3\n 6.3  2.5  5.0  1.9\n 6.5  3.0  5.2  2.0\n 6.2  3.4  5.4  2.3\n 5.9  3.0  5.1  1.8\n\njulia> y = convert(Array, iris[!, :Species])\n150-element Array{String,1}:\n \"setosa\"\n \"setosa\"\n \"setosa\"\n \"setosa\"\n \"setosa\"\n \"setosa\"\n \"setosa\"\n \"setosa\"\n \"setosa\"\n \"setosa\"\n ⋮\n \"virginica\"\n \"virginica\"\n \"virginica\"\n \"virginica\"\n \"virginica\"\n \"virginica\"\n \"virginica\"\n \"virginica\"\n \"virginica\"\n","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"Next, we load the LogisticRegression model from scikit-learn's library. This would require scikit-learn. See Installation","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"julia> using ScikitLearn\n\njulia> @sk_import linear_model: LogisticRegression\nPyObject <class 'sklearn.linear_model._logistic.LogisticRegression'>","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"Every model's constructor accepts hyperparameters (such as regression strength, whether to fit the intercept, the penalty type, etc.) as keyword arguments.  Check out ?LogisticRegression for details.","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"julia> model = LogisticRegression(fit_intercept=true, max_iter = 200)\nPyObject LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\n","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"Then we train the model and evaluate its accuracy on the training set:","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"julia> fit!(model, X, y);\n\njulia> accuracy = score(model, X, y)\n0.9733333333333334\n\njulia> println(\"accuracy: $accuracy\")\naccuracy: 0.9733333333333334\n","category":"page"},{"location":"man/quickstart/#Cross-validation-1","page":"Quick Start Guide","title":"Cross-validation","text":"","category":"section"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"This will train five models, on five train/test splits of X and y, and return the test-set accuracy of each:","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"julia> using ScikitLearn.CrossValidation: cross_val_score\n\njulia> cross_val_score(LogisticRegression(max_iter=130), X, y; cv=5)  # 5-fold\n5-element Array{Float64,1}:\n 0.9666666666666667\n 1.0\n 0.9333333333333333\n 0.9666666666666667\n 1.0\n  ","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"See this tutorial for more information.","category":"page"},{"location":"man/quickstart/#Hyperparameter-tuning-1","page":"Quick Start Guide","title":"Hyperparameter tuning","text":"","category":"section"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"LogisticRegression has a regularization-strength parameter C (smaller is stronger). We can use grid search algorithms to find the optimal C.","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"GridSearchCV will try all values of C in 0.1:0.1:2.0 and will return the one with the highest cross-validation performance.","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"julia> using ScikitLearn.GridSearch: GridSearchCV\n\njulia> gridsearch = GridSearchCV(LogisticRegression(max_iter=200), Dict(:C => 0.1:0.1:2.0))\nGridSearchCV\n  estimator: PyCall.PyObject\n  param_grid: Dict{Symbol,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}\n  scoring: Nothing nothing\n  loss_func: Nothing nothing\n  score_func: Nothing nothing\n  fit_params: Dict{Any,Any}\n  n_jobs: Int64 1\n  iid: Bool true\n  refit: Bool true\n  cv: Nothing nothing\n  verbose: Int64 0\n  error_score: String \"raise\"\n  scorer_: Nothing nothing\n  best_params_: Nothing nothing\n  best_score_: Nothing nothing\n  grid_scores_: Nothing nothing\n  best_estimator_: Nothing nothing\n\njulia> fit!(gridsearch, X, y);\n\njulia> println(\"Best parameters: $(gridsearch.best_params_)\")\nBest parameters: Dict{Symbol,Any}(:C => 0.6)\n","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"Finally, we plot cross-validation accuracy vs. C","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"using RDatasets\niris = dataset(\"datasets\", \"iris\")\nX = convert(Array, iris[!, [:SepalLength, :SepalWidth, :PetalLength, \n        :PetalWidth]])\ny = convert(Array, iris[!,:Species])\n\nusing ScikitLearn\nusing ScikitLearn.GridSearch: GridSearchCV\n@sk_import linear_model: LogisticRegression\n\ngridsearch = GridSearchCV(LogisticRegression(max_iter=200), Dict(:C => 0.1:0.1:2.0))\nfit!(gridsearch, X, y)","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"using PyPlot, Statistics\nplot([cv_res.parameters[:C] for cv_res in gridsearch.grid_scores_],\n     [mean(cv_res.cv_validation_scores) for cv_res in gridsearch.grid_scores_])\nsavefig(\"gridplot.svg\");nothing #hide","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"(Image: )","category":"page"},{"location":"man/quickstart/#Saving-the-model-to-disk-1","page":"Quick Start Guide","title":"Saving the model to disk","text":"","category":"section"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"Both Python and Julia models can be saved to disk","category":"page"},{"location":"man/quickstart/#","page":"Quick Start Guide","title":"Quick Start Guide","text":"julia> import JLD, PyCallJLD\n\njulia> JLD.save(\"my_model.jld\", \"model\", model)\n\njulia> model = JLD.load(\"my_model.jld\", \"model\")    # Load it back\n┌ Warning: type PyCallJLD.PyObjectSerialization not present in workspace; reconstructing\n└ @ JLD ~/.julia/packages/JLD/uVJmd/src/jld_types.jl:722\ngetfield(JLD, Symbol(\"##PyCallJLD.PyObjectSerialization#371\"))(UInt8[0x80, 0x03, 0x63, 0x73, 0x6b, 0x6c, 0x65, 0x61, 0x72, 0x6e  …  0x2e, 0x32, 0x32, 0x2e, 0x31, 0x71, 0x43, 0x75, 0x62, 0x2e])\n","category":"page"},{"location":"man/pipelines/#","page":"Pipelines and FeatureUnion","title":"Pipelines and FeatureUnion","text":"Most data science and machine learning problems involve several steps of data preprocessing and transformation. ScikitLearn.jl provides two types to facilitate this task.","category":"page"},{"location":"man/pipelines/#Pipelines-1","page":"Pipelines and FeatureUnion","title":"Pipelines","text":"","category":"section"},{"location":"man/pipelines/#","page":"Pipelines and FeatureUnion","title":"Pipelines and FeatureUnion","text":"Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification.","category":"page"},{"location":"man/pipelines/#","page":"Pipelines and FeatureUnion","title":"Pipelines and FeatureUnion","text":"julia> using ScikitLearn\n\njulia> using ScikitLearn.Pipelines: Pipeline, make_pipeline\n\njulia> @sk_import decomposition: PCA\nPyObject <class 'sklearn.decomposition._pca.PCA'>\n\njulia> @sk_import linear_model: LogisticRegression \nPyObject <class 'sklearn.linear_model._logistic.LogisticRegression'>\n\njulia> using RDatasets: dataset\n\njulia> iris = dataset(\"datasets\", \"iris\");\n\njulia> X = convert(Array, iris[!, [:SepalLength, :SepalWidth, :PetalLength, :PetalWidth]]);\n\njulia> y = convert(Array, iris[!,:Species]);\n\njulia> estimators = [(\"reduce_dim\", PCA()), (\"logistic_regression\", LogisticRegression())]\n2-element Array{Tuple{String,PyCall.PyObject},1}:\n (\"reduce_dim\", PyObject PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n    svd_solver='auto', tol=0.0, whiten=False))\n (\"logistic_regression\", PyObject LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False))\n\njulia> clf = Pipeline(estimators)\nPipeline(Tuple{Any,Any}[(\"reduce_dim\", PyObject PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n    svd_solver='auto', tol=0.0, whiten=False)), (\"logistic_regression\", PyObject LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False))], Any[PyObject PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n    svd_solver='auto', tol=0.0, whiten=False), PyObject LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)])\n\njulia> fit!(clf, X, y)\nPipeline(Tuple{Any,Any}[(\"reduce_dim\", PyObject PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n    svd_solver='auto', tol=0.0, whiten=False)), (\"logistic_regression\", PyObject LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False))], Any[PyObject PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n    svd_solver='auto', tol=0.0, whiten=False), PyObject LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)])","category":"page"},{"location":"man/pipelines/#","page":"Pipelines and FeatureUnion","title":"Pipelines and FeatureUnion","text":"See ?Pipeline, ?make_pipeline and the user guide for details.","category":"page"},{"location":"man/pipelines/#Examples-1","page":"Pipelines and FeatureUnion","title":"Examples","text":"","category":"section"},{"location":"man/pipelines/#","page":"Pipelines and FeatureUnion","title":"Pipelines and FeatureUnion","text":"Pipelining: chaining a PCA and a logistic regression\nRestricted Boltzmann Machine features for digit classification","category":"page"},{"location":"man/pipelines/#Feature-Unions-1","page":"Pipelines and FeatureUnion","title":"Feature Unions","text":"","category":"section"},{"location":"man/pipelines/#","page":"Pipelines and FeatureUnion","title":"Pipelines and FeatureUnion","text":"FeatureUnion combines several transformer objects into a new transformer that combines their output. A FeatureUnion takes a list of transformer objects. During fitting, each of these is fit to the data independently. For transforming data, the transformers are applied in parallel, and the sample vectors they output are concatenated end-to-end into larger vectors.","category":"page"},{"location":"man/pipelines/#","page":"Pipelines and FeatureUnion","title":"Pipelines and FeatureUnion","text":"julia> using ScikitLearn.Pipelines: FeatureUnion\n\njulia> @sk_import decomposition: KernelPCA\nPyObject <class 'sklearn.decomposition._kernel_pca.KernelPCA'>\n\njulia> estimators = [(\"linear_pca\", PCA()), (\"kernel_pca\", KernelPCA())]\n2-element Array{Tuple{String,PyCall.PyObject},1}:\n (\"linear_pca\", PyObject PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n    svd_solver='auto', tol=0.0, whiten=False))\n (\"kernel_pca\", PyObject KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto',\n          fit_inverse_transform=False, gamma=None, kernel='linear',\n          kernel_params=None, max_iter=None, n_components=None, n_jobs=None,\n          random_state=None, remove_zero_eig=False, tol=0))\n\njulia> combined = FeatureUnion(estimators)\nFeatureUnion(Tuple{Any,Any}[(\"linear_pca\", PyObject PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n    svd_solver='auto', tol=0.0, whiten=False)), (\"kernel_pca\", PyObject KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto',\n          fit_inverse_transform=False, gamma=None, kernel='linear',\n          kernel_params=None, max_iter=None, n_components=None, n_jobs=None,\n          random_state=None, remove_zero_eig=False, tol=0))], 1, nothing)\n","category":"page"},{"location":"man/pipelines/#","page":"Pipelines and FeatureUnion","title":"Pipelines and FeatureUnion","text":"See ?FeatureUnion and the user guide for more.","category":"page"},{"location":"man/pipelines/#Examples-2","page":"Pipelines and FeatureUnion","title":"Examples","text":"","category":"section"},{"location":"man/pipelines/#","page":"Pipelines and FeatureUnion","title":"Pipelines and FeatureUnion","text":"Concatenating multiple feature extraction methods","category":"page"},{"location":"man/models/#Models-1","page":"Models","title":"Models","text":"","category":"section"},{"location":"man/models/#Python-models-1","page":"Models","title":"Python models","text":"","category":"section"},{"location":"man/models/#","page":"Models","title":"Models","text":"scikit-learn has on the order of 100 to 200 models (more generally called \"estimators\"), split into three categories:","category":"page"},{"location":"man/models/#","page":"Models","title":"Models","text":"Supervised Learning (linear regression, support vector machines, random forest, neural nets, ...)\nUnsupervised Learning (clustering, PCA, mixture models, manifold learning, ...)\nDataset Transformation (preprocessing, text feature extraction, one-hot encoding, ...)","category":"page"},{"location":"man/models/#","page":"Models","title":"Models","text":"All of those estimators will work with ScikitLearn.jl. They are imported with @sk_import. For example, here's how to import and fit sklearn.linear_regression.LogisticRegression","category":"page"},{"location":"man/models/#","page":"Models","title":"Models","text":"julia> using ScikitLearn, Random\n\njulia> Random.seed!(11); #ensures reproducibility\n\njulia> X = rand(20,3); y = rand([true, false], 20);\n\njulia> @sk_import linear_model: LogisticRegression\nPyObject <class 'sklearn.linear_model._logistic.LogisticRegression'>\n\njulia> using ScikitLearn.CrossValidation: train_test_split\n\njulia> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42);\n\njulia> log_reg = fit!(LogisticRegression(penalty=\"l2\"), X_train, y_train);\n\njulia> predict(log_reg, X_test)\n5-element Array{Bool,1}:\n 0\n 0\n 0\n 0\n 0\n","category":"page"},{"location":"man/models/#","page":"Models","title":"Models","text":"Reminder: ?LogisticRegression contains a lot of information about the model parameters.","category":"page"},{"location":"man/models/#Installation-1","page":"Models","title":"Installation","text":"","category":"section"},{"location":"man/models/#","page":"Models","title":"Models","text":"Importing the Python models requires Python 3.x with numpy, and the scikit-learn library. This is easiest to get through Conda.jl, which is already installed on your system.  Calling @sk_import linear_model: LinearRegression should automatically install everything. You can also install scikit-learn manually with Conda.add(\"scikit-learn\"). If you have other issues, please refer to PyCall.jl, or post an issue","category":"page"},{"location":"man/models/#Julia-models-1","page":"Models","title":"Julia models","text":"","category":"section"},{"location":"man/models/#","page":"Models","title":"Models","text":"Julia models are hosted in other packages, and need to be installed separately with Pkg.add or Pkg.checkout (to get the latest version - sometimes necessary). They all implement the common api, and provide hyperparameter information in their ?docstrings.","category":"page"},{"location":"man/models/#","page":"Models","title":"Models","text":"note: Note\nUnfortunately, some packages export a fit! function that conflicts with ScikitLearn's fit!. This can be fixed by adding this line:using ScikitLearn: fit!, predict\n","category":"page"},{"location":"man/models/#ScikitLearn-in-built-models-1","page":"Models","title":"ScikitLearn in-built models","text":"","category":"section"},{"location":"man/models/#","page":"Models","title":"Models","text":"ScikitLearn.Models.LinearRegression() implements linear regression using \\, optimized for speed. See ?LinearRegression for fitting options.","category":"page"},{"location":"man/models/#GaussianMixtures.jl-1","page":"Models","title":"GaussianMixtures.jl","text":"","category":"section"},{"location":"man/models/#","page":"Models","title":"Models","text":"julia> using GaussianMixtures: GMM #remember to install package first\n\njulia> gmm = fit!(GMM(n_components=3, kind=:diag), X_train);\n[ Info: Initializing GMM, 3 Gaussians diag covariance 3 dimensions using 15 data points\n  Iters               objv        objv-change | affected\n-------------------------------------------------------------\n      0       1.462249e+00\n      1       1.041033e+00      -4.212161e-01 |        2\n      2       9.589243e-01      -8.210827e-02 |        2\n      3       9.397430e-01      -1.918135e-02 |        0\n      4       9.397430e-01       0.000000e+00 |        0\nK-means converged with 4 iterations (objv = 0.9397430000827904)\n┌ Info: K-means with 15 data points using 4 iterations\n└ 1.3 data points per parameter\n\njulia> predict_proba(gmm, X_test)\n5×3 Array{Float64,2}:\n 1.37946e-7   5.58899e-9   1.0\n 0.986895     1.98749e-10  0.0131053\n 0.998037     1.00296e-15  0.00196321\n 2.66238e-11  0.041746     0.958254\n 0.999984     4.05443e-6   1.16204e-5\n","category":"page"},{"location":"man/models/#","page":"Models","title":"Models","text":"Documentation at GaussianMixtures.jl. Example: density estimation","category":"page"},{"location":"man/models/#GaussianProcesses.jl-1","page":"Models","title":"GaussianProcesses.jl","text":"","category":"section"},{"location":"man/models/#","page":"Models","title":"Models","text":"julia> using GaussianProcesses: GPE, MeanZero, SE #remember to install package first\n\njulia> gp = fit!(GPE(; mean=MeanZero(), kernel=SE(0.0, 0.0), logNoise=-1e8), X_train, Float64.(y_train))\nGP Exact object:\n  Dim = 3\n  Number of observations = 15\n  Mean function:\n    Type: MeanZero, Params: Float64[]\n  Kernel:\n    Type: GaussianProcesses.SEIso{Float64}, Params: [0.0, 0.0]\n  Input observations =\n[0.376913304113047 0.5630896022795546 … 0.31598998347835017 0.5828199336036355; 0.50060556533132 0.4124482236437548 … 0.6750380496244157 0.6147514739028759; 0.5142063690337368 0.4774433498612982 … 0.9823652195180261 0.21010382988916376]\n  Output observations = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]\n  Variance of observation noise = 0.0\n  Marginal Log-Likelihood = -749.473\n\njulia> predict(gp, X_test)\n5-element Array{Float64,1}:\n  2.1522493172851114\n  1.298965158590363\n  0.8142639915887457\n -0.7287701449370729\n  0.7495235968268048\n","category":"page"},{"location":"man/models/#","page":"Models","title":"Models","text":"Documentation at GaussianProcesses.jl and in the ?GPE docstring. Example: Gaussian Processes","category":"page"},{"location":"man/models/#","page":"Models","title":"Models","text":"Gaussian Processes have a lot of hyperparameters, see get_params(GP) for a list. They can all be tuned","category":"page"},{"location":"man/models/#DecisionTree.jl-1","page":"Models","title":"DecisionTree.jl","text":"","category":"section"},{"location":"man/models/#","page":"Models","title":"Models","text":"DecisionTreeClassifier\nDecisionTreeRegressor\nRandomForestClassifier\nRandomForestRegressor\nAdaBoostStumpClassifier","category":"page"},{"location":"man/models/#","page":"Models","title":"Models","text":"Documentation at DecisionTree.jl. Examples: Classifier Comparison, Decision Tree Regression notebooks.","category":"page"},{"location":"man/models/#LowRankModels.jl-1","page":"Models","title":"LowRankModels.jl","text":"","category":"section"},{"location":"man/models/#","page":"Models","title":"Models","text":"SkGLRM: Generalized Low Rank Model\nPCA: Principal Component Analysis\nQPCA: Quadratically Regularized PCA\nRPCA: Robust PCA\nNNMF: Non-negative matrix factorization\nKMeans: The k-means algorithm","category":"page"},{"location":"man/models/#","page":"Models","title":"Models","text":"note: Note\nThese algorithms are all special cases of the Generalized Low Rank Model algorithm, whose main goal is to provide flexible loss and regularization for heterogeneous data.  Specialized algorithms will achieve faster convergence in general. Documentation at LowRankModels.jl. Example: KMeans Digit Classifier.","category":"page"},{"location":"man/models/#Contributing-1","page":"Models","title":"Contributing","text":"","category":"section"},{"location":"man/models/#","page":"Models","title":"Models","text":"To make your Julia model compatible with ScikitLearn.jl, you need to implement the scikit-learn interface. See ScikitLearnBase.jl","category":"page"},{"location":"man/model_selection/#Model-Selection-1","page":"Model Selection","title":"Model Selection","text":"","category":"section"},{"location":"man/model_selection/#Grid-Search-1","page":"Model Selection","title":"Grid Search","text":"","category":"section"},{"location":"man/model_selection/#","page":"Model Selection","title":"Model Selection","text":"Most models contain hyperparameters: parameters that are specified in the constructor, and not learned from the data. ScikitLearn.jl provides GridSearchCV to find the best set of hyper-parameter:","category":"page"},{"location":"man/model_selection/#","page":"Model Selection","title":"Model Selection","text":"julia> using ScikitLearn, Random\n\njulia> Random.seed!(11);\n\njulia> X = rand(25,4); y = rand([true,false], 25);\n\njulia> @sk_import linear_model: LogisticRegression;\n\njulia> using ScikitLearn.GridSearch: GridSearchCV\n\njulia> gridsearch = GridSearchCV(LogisticRegression(max_iter=200), Dict(:C => 0.1:0.1:2.0))\nGridSearchCV\n  estimator: PyCall.PyObject\n  param_grid: Dict{Symbol,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}\n  scoring: Nothing nothing\n  loss_func: Nothing nothing\n  score_func: Nothing nothing\n  fit_params: Dict{Any,Any}\n  n_jobs: Int64 1\n  iid: Bool true\n  refit: Bool true\n  cv: Nothing nothing\n  verbose: Int64 0\n  error_score: String \"raise\"\n  scorer_: Nothing nothing\n  best_params_: Nothing nothing\n  best_score_: Nothing nothing\n  grid_scores_: Nothing nothing\n  best_estimator_: Nothing nothing\n\n\n\njulia> fit!(gridsearch, X, y)\nGridSearchCV\n  estimator: PyCall.PyObject\n  param_grid: Dict{Symbol,StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}}\n  scoring: Nothing nothing\n  loss_func: Nothing nothing\n  score_func: Nothing nothing\n  fit_params: Dict{Any,Any}\n  n_jobs: Int64 1\n  iid: Bool true\n  refit: Bool true\n  cv: Nothing nothing\n  verbose: Int64 0\n  error_score: String \"raise\"\n  scorer_: score (function of type typeof(score))\n  best_params_: Dict{Symbol,Any}\n  best_score_: Float64 0.6\n  grid_scores_: Array{ScikitLearn.Skcore.CVScoreTuple}((20,))\n  best_estimator_: PyCall.PyObject\n\n\n\njulia> println(\"Best hyper-parameters: $(gridsearch.best_params_)\")\nBest hyper-parameters: Dict{Symbol,Any}(:C => 0.8)\n","category":"page"},{"location":"man/model_selection/#","page":"Model Selection","title":"Model Selection","text":"See ?GridSearchCV and the scikit-learn docs for details.","category":"page"},{"location":"man/model_selection/#Examples-1","page":"Model Selection","title":"Examples","text":"","category":"section"},{"location":"man/model_selection/#","page":"Model Selection","title":"Model Selection","text":"Quick start guide\nPipelining: chaining a PCA and a logistic regression\nConcatenating multiple feature extraction methods","category":"page"},{"location":"man/model_selection/#Random-Search-1","page":"Model Selection","title":"Random Search","text":"","category":"section"},{"location":"man/model_selection/#","page":"Model Selection","title":"Model Selection","text":"RandomizedSearchCV will sample from each parameter independently. Documentation here.","category":"page"},{"location":"man/model_selection/#","page":"Model Selection","title":"Model Selection","text":"Note: The distributions have to be specified using scipy.stats (see example below), but we hope to support Distributions.jl in the future. File an issue if this is a pain point.","category":"page"},{"location":"man/model_selection/#Examples-2","page":"Model Selection","title":"Examples","text":"","category":"section"},{"location":"man/model_selection/#","page":"Model Selection","title":"Model Selection","text":"Comparing randomized search and grid search for hyperparameter estimation","category":"page"},{"location":"man/help/#Getting-help-1","page":"Getting Help","title":"Getting help","text":"","category":"section"},{"location":"man/help/#","page":"Getting Help","title":"Getting Help","text":"Discourse is a nice place to ask questions about ScikitLearn.jl. Please file an issue for any problem you encounter related to the Julia interface. If your question is about the behaviour of a model that was imported through @sk_import, or about machine learning in general, consider posting on Stack Overflow","category":"page"},{"location":"man/examples_refer/#Examples-1","page":"Example Gallery","title":"Examples","text":"","category":"section"},{"location":"man/examples_refer/#","page":"Example Gallery","title":"Example Gallery","text":"See here","category":"page"},{"location":"#Introduction-1","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"The scikit-learn Python library has proven very popular with machine learning researchers and data scientists in the last five years. It provides a uniform interface for training and using models, as well as a set of tools for chaining (pipelines), evaluating, and tuning model hyperparameters. ScikitLearn.jl brings these capabilities to Julia.  Its primary goal is to integrate both Julia- and Python-defined models together into the scikit-learn framework.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"This library uses docstrings extensively (type ?function_name at the REPL). The primary source of information, tutorial, etc. is at scikit-learn, but please see how to read the Python documentation beforehand.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Note: ScikitLearn.jl is not associated to the scikit-learn organization. All issues should be filed here","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"Credits: this code and documentation was adapted from Paul Butler's sklearn-pandas","category":"page"},{"location":"man/dataframes/#DataFrames-1","page":"DataFrames","title":"DataFrames","text":"","category":"section"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"It is possible to use a dataframe as a training set, but it needs to be converted to an array first. DataFrameMapper is used to specify how this conversion proceeds. For example, PCA might be applied to some numerical dataframe columns, and one-hot-encoding to a categorical column.","category":"page"},{"location":"man/dataframes/#Transformation-Mapping-1","page":"DataFrames","title":"Transformation Mapping","text":"","category":"section"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"Consider this dataset:","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"julia> using ScikitLearn\n\njulia> using DataFrames: DataFrame, missing\n\njulia> @sk_import preprocessing: (LabelBinarizer, StandardScaler)\nPyObject <class 'sklearn.preprocessing._data.StandardScaler'>\n\njulia> data = DataFrame(pet=[\"cat\", \"dog\", \"dog\", \"fish\", \"cat\", \"dog\", \"cat\", \"fish\"],\n                 children=[4., 6, 3, 3, 2, 3, 5, 4],\n                 salary=[90, 24, 44, 27, 32, 59, 36, 27])\n8×3 DataFrames.DataFrame\n│ Row │ pet    │ children │ salary │\n│     │ String │ Float64  │ Int64  │\n├─────┼────────┼──────────┼────────┤\n│ 1   │ cat    │ 4.0      │ 90     │\n│ 2   │ dog    │ 6.0      │ 24     │\n│ 3   │ dog    │ 3.0      │ 44     │\n│ 4   │ fish   │ 3.0      │ 27     │\n│ 5   │ cat    │ 2.0      │ 32     │\n│ 6   │ dog    │ 3.0      │ 59     │\n│ 7   │ cat    │ 5.0      │ 36     │\n│ 8   │ fish   │ 4.0      │ 27     │\n\n","category":"page"},{"location":"man/dataframes/#Map-the-Columns-to-Transformations-1","page":"DataFrames","title":"Map the Columns to Transformations","text":"","category":"section"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"The mapper takes a list of pairs. The first is a column name from the DataFrame, or a list containing one or multiple columns (we will see an example with multiple columns later). The second is an object which will perform the transformation which will be applied to that column:","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"Note: ScikitLearn.DataFrameMapper won't be available until DataFrames is imported","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"julia> mapper = DataFrameMapper([(:pet, LabelBinarizer()),\n                          ([:children], StandardScaler())]);","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"The difference between specifying the column selector as :column (as a single symbol) and [:column] (as a list with one element) is the shape of the array that is passed to the transformer. In the first case, a one dimensional array with be passed, while in the second case it will be a 2-dimensional array with one column, i.e. a column vector.","category":"page"},{"location":"man/dataframes/#Test-the-Transformation-1","page":"DataFrames","title":"Test the Transformation","text":"","category":"section"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"We can use the fit_transform! shortcut to both fit the model and see what transformed data looks like. In this and the other examples, output is rounded to two digits with round to account for rounding errors on different hardware:","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"julia> round.(fit_transform!(mapper, copy(data)), digits=2)\n8×4 Array{Float64,2}:\n 1.0  0.0  0.0   0.21\n 0.0  1.0  0.0   1.88\n 0.0  1.0  0.0  -0.63\n 0.0  0.0  1.0  -0.63\n 1.0  0.0  0.0  -1.46\n 0.0  1.0  0.0  -0.63\n 1.0  0.0  0.0   1.04\n 0.0  0.0  1.0   0.21\n","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"Note that the first three columns are the output of the LabelBinarizer (corresponding to cat, dog, and fish  respectively) and the fourth column is the standardized value for the number of children. In general, the columns are ordered according to the order given when the DataFrameMapper is constructed.","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"Now that the transformation is trained, we confirm that it works on new data:","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"julia> sample = DataFrame(pet = [\"cat\"], children = [5.])\n1×2 DataFrames.DataFrame\n│ Row │ pet    │ children │\n│     │ String │ Float64  │\n├─────┼────────┼──────────┤\n│ 1   │ cat    │ 5.0      │\n\njulia> round.(transform(mapper, sample), digits=2)\n1×4 Array{Float64,2}:\n 1.0  0.0  0.0  1.04\n","category":"page"},{"location":"man/dataframes/#Transform-Multiple-Columns-1","page":"DataFrames","title":"Transform Multiple Columns","text":"","category":"section"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"Transformations may require multiple input columns. In these cases, the column names can be specified in a list:","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"julia> @sk_import decomposition: PCA\nPyObject <class 'sklearn.decomposition._pca.PCA'>\n\njulia> mapper2 = DataFrameMapper([([:children, :salary], PCA(1))]);\n","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"Now running fit_transform! will run PCA on the children and salary columns and return the first principal component:","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"julia> round.(fit_transform!(mapper2, copy(data)), digits=1)\n8×1 Array{Float64,2}:\n  47.6\n -18.4\n   1.6\n -15.4\n -10.4\n  16.6\n  -6.4\n -15.4\n","category":"page"},{"location":"man/dataframes/#Multiple-transformers-for-the-same-column-1","page":"DataFrames","title":"Multiple transformers for the same column","text":"","category":"section"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"Multiple transformers can be applied to the same column specifying them in a list:","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"julia> @sk_import impute: SimpleImputer\nPyObject <class 'sklearn.impute._base.SimpleImputer'>\n\njulia> mapper3 = DataFrameMapper([([:age], [SimpleImputer(),\n                                     StandardScaler()])]; missing2NaN=true);\n\njulia> data_3 = DataFrame(age= [1, missing, 3]);\n\njulia> fit_transform!(mapper3, data_3)\n3×1 Array{Float64,2}:\n -1.224744871391589\n  0.0\n  1.224744871391589\n","category":"page"},{"location":"man/dataframes/#Columns-that-don't-need-any-transformation-1","page":"DataFrames","title":"Columns that don't need any transformation","text":"","category":"section"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"Only columns that are listed in the DataFrameMapper are kept. To keep a column but don't apply any transformation to it, use nothing as transformer:","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"julia> mapper3 = DataFrameMapper([(:pet, LabelBinarizer()), (:children, nothing)]);\n\njulia> round.(fit_transform!(mapper3, copy(data)))\n8×4 Array{Float64,2}:\n 1.0  0.0  0.0  4.0\n 0.0  1.0  0.0  6.0\n 0.0  1.0  0.0  3.0\n 0.0  0.0  1.0  3.0\n 1.0  0.0  0.0  2.0\n 0.0  1.0  0.0  3.0\n 1.0  0.0  0.0  5.0\n 0.0  0.0  1.0  4.0\n","category":"page"},{"location":"man/dataframes/#Cross-validation-1","page":"DataFrames","title":"Cross-validation","text":"","category":"section"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"Now that we can combine features from a DataFrame, we may want to use cross-validation to see whether our model works.","category":"page"},{"location":"man/dataframes/#","page":"DataFrames","title":"DataFrames","text":"julia> @sk_import linear_model: LinearRegression\nPyObject <class 'sklearn.linear_model._base.LinearRegression'>\n\njulia> using ScikitLearn.CrossValidation: cross_val_score\n\njulia> pipe = Pipelines.Pipeline([(:featurize, mapper), (:lm, LinearRegression())]);\n\njulia> round.(cross_val_score(pipe, data, data[!,:salary]), digits=2)\n3-element Array{Float64,1}:\n  -1.09\n  -5.3\n -15.38\n","category":"page"}]
}
